import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler

# âœ… Mac GPU(MPS) ì§€ì› ì—¬ë¶€ ì²´í¬ & ì„¤ì •
if torch.backends.mps.is_available():
    device = torch.device("mps")  # Metal API (Mac GPU)
    print("ğŸš€ Using device: Mac GPU (Metal MPS)")
else:
    device = torch.device("cpu")  # MPSê°€ ì•ˆë˜ë©´ CPU ì‚¬ìš©
    print("âš ï¸ MPS ì§€ì›ë˜ì§€ ì•ŠìŒ. CPU ì‚¬ìš©")

# âœ… ì•ˆì „í•œ eval ë³€í™˜ í•¨ìˆ˜ (ì˜¤ë¥˜ ë°©ì§€)
def safe_eval(x):
    try:
        return eval(x) if isinstance(x, str) else np.nan
    except Exception as e:
        print(f"âŒ `safe_eval()` ë³€í™˜ ì‹¤íŒ¨: {x} | ì˜¤ë¥˜: {e}")
        return np.nan

# âœ… ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (LSTMìš©)
def load_lstm_data(folder_path):
    all_files = [f for f in os.listdir(folder_path) if f.endswith(".csv")]
    data_frames = []

    for file in all_files:
        file_path = os.path.join(folder_path, file)
        df = pd.read_csv(file_path)

        # âœ… ì»¬ëŸ¼ í™•ì¸ (ë””ë²„ê¹…)
        # print(f"ğŸ“‚ íŒŒì¼: {file} | í¬í•¨ëœ ì»¬ëŸ¼: {df.columns.tolist()}")

        # âœ… ë‚ ì§œ ë³€í™˜ ë° ì •ë ¬
        df["start_date"] = pd.to_datetime(df["start_date"])
        df = df.sort_values(by=["symbol", "start_date"])  # ì•”í˜¸í™”íë³„ ì‹œê°„ìˆœ ì •ë ¬

        # âœ… actual_frequencies ë³€í™˜
        if "actual_frequencies" in df.columns:
            df["actual_frequencies"] = df["actual_frequencies"].apply(safe_eval)

        # âœ… First Digit ë³€í™˜ (F1~F9)
        first_mask = df["digit_type"] == "first"
        if first_mask.any():
            first_features = pd.DataFrame(
                df.loc[first_mask, "actual_frequencies"].tolist(),
                columns=[f"F{i+1}" for i in range(9)],
                index=df.loc[first_mask].index
            )
            df = df.join(first_features)

        # âœ… Second Digit ë³€í™˜ (S0~S9)
        second_mask = df["digit_type"] == "second"
        if second_mask.any():
            second_features = pd.DataFrame(
                df.loc[second_mask, "actual_frequencies"].tolist(),
                columns=[f"S{i}" for i in range(10)],
                index=df.loc[second_mask].index
            )
            df = df.join(second_features)

        # âœ… Feature ì»¬ëŸ¼ ì„¤ì •
        feature_cols = ["mad"] + [f"F{i+1}" for i in range(9)] + [f"S{i}" for i in range(10)]

        # âœ… ëˆ„ë½ëœ ì»¬ëŸ¼ í™•ì¸
        missing_cols = [col for col in feature_cols if col not in df.columns]
        if missing_cols:
            print(f"âŒ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_cols} | íŒŒì¼: {file}")

        # âœ… ìµœì¢… Feature ì •ë¦¬
        df = df[["symbol", "start_date"] + feature_cols]

        data_frames.append(df)

    return pd.concat(data_frames, ignore_index=True)

# âœ… LSTM ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ì˜¤ë¥˜ ë°©ì§€ ì½”ë“œ ì¶”ê°€)
class CryptoTimeSeriesDataset(Dataset):
    def __init__(self, df, sequence_lengths=[15, 20, 25, 30]):
        self.sequence_lengths = sequence_lengths
        self.grouped_data = {symbol: df_group for symbol, df_group in df.groupby("symbol")}

        # âœ… ì „ì²´ ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥
        self.X, self.y = [], []

        # âœ… ê° ì½”ì¸ì˜ ë°ì´í„°ë¥¼ ì‹œê³„ì—´ í˜•íƒœë¡œ ë³€í™˜
        for symbol, group in self.grouped_data.items():
            features = group.iloc[:, 2:].values  # symbol, date ì œì™¸
            num_features = features.shape[1]  # feature ê°œìˆ˜

            # âœ… ëª¨ë“  ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ë¡œ í™•ì¸
            for seq_length in self.sequence_lengths:
                for i in range(len(features) - seq_length):
                    seq = features[i:i + seq_length]  # (seq_length, num_features)

                    # âœ… ì‹œí€€ìŠ¤ í¬ê¸° ì²´í¬ (ì •í™•í•œ shape ë³´ì¥)
                    if seq.shape == (seq_length, num_features):
                        self.X.append(seq)
                        self.y.append(features[i + seq_length][-1])  # ë§ˆì§€ë§‰ ê°’ ì˜ˆì¸¡

        # âœ… NumPy ë°°ì—´ ë³€í™˜ (ì˜¤ë¥˜ ë°©ì§€)
        self.X = np.array(self.X, dtype=np.float32)  # (samples, seq_length, features)
        self.y = np.array(self.y, dtype=np.float32)  # (samples,)

        print(f"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ! ì´ ìƒ˜í”Œ ìˆ˜: {len(self.X)}")

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)

# âœ… LSTM ëª¨ë¸ ì •ì˜
class LSTMPredictor(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(LSTMPredictor, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_out = lstm_out[:, -1, :]  # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì¶œë ¥ê°’
        out = self.fc(last_out)
        return self.sigmoid(out)  # í™•ë¥ ê°’ ë°˜í™˜

# âœ… ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
def train_model(model, train_loader, criterion, optimizer, num_epochs=10):
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            y_pred = model(X_batch).squeeze()
            loss = criterion(y_pred, y_batch)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
        
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}")

# âœ… ì‹¤í–‰
def main():
    data_folder = "./crypto_data/TraingData/Total_CSV/1.1_BN_Train/"
    df = load_lstm_data(data_folder)

    # âœ… Dataset & DataLoader ìƒì„± (ë‹¤ì–‘í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ ì ìš©)
    sequence_lengths = [15, 20, 25, 30]
    dataset = CryptoTimeSeriesDataset(df, sequence_lengths)
    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

    print(f"âœ… ì´ {len(dataset)}ê°œ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì™„ë£Œ!")

    # âœ… LSTM ëª¨ë¸ ì´ˆê¸°í™”
    input_dim = 21  # mad (first + second), F1~F9, S0~S9 (ì´ 21ê°œ feature)
    hidden_dim = 64
    num_layers = 2
    output_dim = 1

    model = LSTMPredictor(input_dim, hidden_dim, num_layers, output_dim).to(device)

    # âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
    criterion = nn.BCELoss()  # Binary Cross Entropy Loss
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # âœ… ëª¨ë¸ í•™ìŠµ
    train_model(model, train_loader, criterion, optimizer, num_epochs=10)

    # âœ… ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), "lstm_model_mac.pth")
    print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: lstm_model_mac.pth")

# ì‹¤í–‰
if __name__ == "__main__":
    main()
